\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{sec:introduction}{{1}{1}{Introduction}{section.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Probability density functions of the three reference distributions on $(0,1)$. Left: Truncated normal $\mathcal  {N}(0.5, 1/36)$ showing unimodal concentration at the center. Center: Arcsine distribution (Beta$(\tfrac  {1}{2},\tfrac  {1}{2})$) concentrates probability mass near the boundaries. Right: Uniform distribution $\mathcal  {U}(0,1)$ with constant density.}}{1}{figure.1}\protected@file@percent }
\newlabel{fig:pdfs}{{1}{1}{Probability density functions of the three reference distributions on $(0,1)$. Left: Truncated normal $\mathcal {N}(0.5, 1/36)$ showing unimodal concentration at the center. Center: Arcsine distribution (Beta$(\tfrac {1}{2},\tfrac {1}{2})$) concentrates probability mass near the boundaries. Right: Uniform distribution $\mathcal {U}(0,1)$ with constant density}{figure.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Methodology}{2}{section.2}\protected@file@percent }
\newlabel{sec:methodology}{{2}{2}{Methodology}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Sampling and Empirical Distribution Estimation}{2}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Distance Computation}{2}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Experimental Design}{2}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Results and Analysis}{3}{section.3}\protected@file@percent }
\newlabel{sec:results}{{3}{3}{Results and Analysis}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Empirical Distribution Convergence}{3}{subsection.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Empirical histograms and analytical PDFs for three distributions at increasing sample sizes. Rows correspond to sample sizes $n=100$, $n=1{,}000$, and $n=10{,}000$ (top to bottom), while columns correspond to the Truncated Normal, Arcsine, and Uniform distributions (left to right). Each subplot uses an independent axis scale.}}{3}{figure.2}\protected@file@percent }
\newlabel{fig:empirical_convergence}{{2}{3}{Empirical histograms and analytical PDFs for three distributions at increasing sample sizes. Rows correspond to sample sizes $n=100$, $n=1{,}000$, and $n=10{,}000$ (top to bottom), while columns correspond to the Truncated Normal, Arcsine, and Uniform distributions (left to right). Each subplot uses an independent axis scale}{figure.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Problem 1: K-S Statistic Variation with Sample Size}{3}{subsection.3.2}\protected@file@percent }
\newlabel{subsec:problem1}{{3.2}{3}{Problem 1: K-S Statistic Variation with Sample Size}{subsection.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Kolmogorov-Smirnov statistic versus sample size on log-log scale. Curves show mean values over 100 trials for matching empirical and reference distributions. Shaded bands represent $\pm 1$ standard deviation. The dashed black line shows theoretical $O(1/\sqrt  {n})$ decay.}}{4}{figure.3}\protected@file@percent }
\newlabel{fig:ks_decay}{{3}{4}{Kolmogorov-Smirnov statistic versus sample size on log-log scale. Curves show mean values over 100 trials for matching empirical and reference distributions. Shaded bands represent $\pm 1$ standard deviation. The dashed black line shows theoretical $O(1/\sqrt {n})$ decay}{figure.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Problem 2: K-S Confusion Matrices}{4}{subsection.3.3}\protected@file@percent }
\newlabel{subsec:problem2}{{3.3}{4}{Problem 2: K-S Confusion Matrices}{subsection.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces True K-S confusion matrix computed from reference vs reference distributions. This represents the theoretical maximum distinguishability between distributions.}}{4}{figure.4}\protected@file@percent }
\newlabel{fig:true_ks}{{4}{4}{True K-S confusion matrix computed from reference vs reference distributions. This represents the theoretical maximum distinguishability between distributions}{figure.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces K-S statistic confusion matrices for sample sizes $n = 10^2$, $10^3$, and $10^4$. Rows: empirical distributions; columns: reference distributions. Darker shades indicate larger distances. Values shown are means over 100 trials.}}{5}{figure.5}\protected@file@percent }
\newlabel{fig:ks_confusion}{{5}{5}{K-S statistic confusion matrices for sample sizes $n = 10^2$, $10^3$, and $10^4$. Rows: empirical distributions; columns: reference distributions. Darker shades indicate larger distances. Values shown are means over 100 trials}{figure.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Problem 3: Bhattacharyya and Hellinger Distance Confusion Matrices}{5}{subsection.3.4}\protected@file@percent }
\newlabel{subsec:problem3}{{3.4}{5}{Problem 3: Bhattacharyya and Hellinger Distance Confusion Matrices}{subsection.3.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Convergence analysis for histogram bin selection. Matrix error (Frobenius norm) between empirical and true distance matrices as a function of bin count. Optimal bin counts minimize estimation error: 69 bins for Bhattacharyya, 183 bins for Hellinger distances.}}{6}{figure.6}\protected@file@percent }
\newlabel{fig:bin_convergence}{{6}{6}{Convergence analysis for histogram bin selection. Matrix error (Frobenius norm) between empirical and true distance matrices as a function of bin count. Optimal bin counts minimize estimation error: 69 bins for Bhattacharyya, 183 bins for Hellinger distances}{figure.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces True Bhattacharyya (left) and Hellinger (right) distance confusion matrices computed from reference vs reference distributions.}}{6}{figure.7}\protected@file@percent }
\newlabel{fig:true_distances}{{7}{6}{True Bhattacharyya (left) and Hellinger (right) distance confusion matrices computed from reference vs reference distributions}{figure.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Bhattacharyya (left) and Hellinger (right) distance confusion matrices for $n = 10^4$. Rows: empirical distributions; columns: reference distributions. Values shown are means over 100 trials using optimal bin counts (69 for Bhattacharyya, 183 for Hellinger).}}{7}{figure.8}\protected@file@percent }
\newlabel{fig:distance_confusion}{{8}{7}{Bhattacharyya (left) and Hellinger (right) distance confusion matrices for $n = 10^4$. Rows: empirical distributions; columns: reference distributions. Values shown are means over 100 trials using optimal bin counts (69 for Bhattacharyya, 183 for Hellinger)}{figure.8}{}}
\gdef \@abspage@last{7}
